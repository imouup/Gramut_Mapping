import numpy as np
import colour
import matplotlib.pyplot as plt
import matplotlib
from toolkits import wavelength_2_xyz, plot_xyz_color_vectors, plot_gamut_on_chromaticity_diagram
matplotlib.use('Qt5Agg')
import torch
from datetime import datetime
import time
import torch.nn as nn
import torch.optim as optim
from torch.utils.checkpoint import checkpoint
from torch.utils.data import Dataset, DataLoader

'''
è¿™é‡Œæ˜¯ä¸¤ä¸ªç¤ºä¾‹çš„å››åŸºè‰²å’Œäº”åŸºè‰²ç©ºé—´ï¼Œä¸ºäº†æ–¹ä¾¿ç›´æ¥ç”¨äº†æ³¢é•¿æ¥åˆ›å»º
å®é™…ä½¿ç”¨æ—¶è¯·ä½¿ç”¨XYZåæ ‡
'''

def xyY2XYZ(xyY):
    '''
    è¯¥å‡½æ•°è®²xyYåæ ‡è½¬ä¸ºXYZåæ ‡

    :param xyY: xyYåæ ‡ï¼Œ(n,3) numpyæ•°ç»„
    :return: XYZåæ ‡ï¼Œ(n,3) numpyæ•°ç»„
    '''
    XYZ = colour.xyY_to_XYZ(xyY)
    return XYZ

def p42XYZ(p4):
    return p4 @ XYZ4

def p52XYZ(p5):
    return p5 @ XYZ5

def four2five(cordinate, XYZ4, XYZ5):
    '''
    åˆ©ç”¨ä¼ªé€†ï¼Œå°†å››åŸºè‰²ä¸‹çš„åæ ‡è½¬ä¸ºäº”åŸºè‰²ä¸‹çš„åæ ‡

    :param cordinate: (n,4) çš„çŸ©é˜µï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªå››åŸºè‰²ä¸‹çš„åæ ‡
    :param XYZ4: å››åŸºè‰²è½¬æ¢çŸ©é˜µ (3,4)
    :param XYZ5: äº”åŸºè‰²è½¬æ¢çŸ©é˜µ (3,5)
    :return: P_5.T   # (n,5)çš„åˆ©ç”¨æœ€å°äºŒä¹˜æ³•æ˜ å°„åçš„æ— äº”åŸºè‰²ä¸‹çš„åæ ‡
    '''
    # å…ˆå°†è¾“å…¥çš„å››åŸºè‰²é¢œè‰²åœ¨XYZä¸‹è¡¨ç¤º
    cordinate = cordinate.T
    mat4 = XYZ4.T
    xyz = mat4 @ cordinate # cordiante æ˜¯(4,)çš„åˆ—å‘é‡

    # æ±‚äº”åŸºè‰²çŸ©é˜µçš„ä¼ªé€†
    mat5 = XYZ5.T
    mat5_inv = np.linalg.pinv(mat5)
    P_5 = mat5_inv @ xyz
    return P_5.T   # (n,5)

def flag(points4, points5):
    '''
    æ ‡è®°å‡½æ•°

    :param points4: (n,4) å››åŸºè‰²ä¸‹çš„ç‚¹
    :param points5: (n,5) æ±‚ä¼ªé€†åäº”åŸºè‰²ä¸‹çš„ç‚¹
    :return: ç¬¬ä¸€ä¸ªè¿”å›çš„æ ‡è®°åçš„points4ï¼Œç¬¬äºŒä¸ªæ˜¯æ ‡è®°åçš„points5
    '''
    flag = np.any((points5 < 0) | (points5 > 1), axis=1)
    flag = flag[:,np.newaxis]  # æŠŠä¸€ç»´æ•°ç»„è½¬ä¸ºäºŒç»´åˆ—å‘é‡
    P_flags5 = np.hstack((points5,flag))
    P_flags4 = np.hstack((points4,flag))
    return P_flags4, P_flags5


def GetPoints4(n_samples=1000, seed=233):
    '''
    åœ¨å››åŸºè‰²ä¸‹å‡åŒ€å–ç‚¹

    :return: ä¸€ä¸ªn_samplesè¡Œä¸‰åˆ—çš„çŸ©é˜µ,è¿”å›çš„ç‚¹åœ¨BT2020å®šä¹‰çš„åæ ‡ç³»ä¸‹
    '''
    np.random.seed(seed)
    points = np.random.rand(n_samples, 4)
    return points


def filter(points):
    '''
    åˆ†ç¦»flag = 0ä¸flag =1çš„ç‚¹

    :param points:  four2fiveæ•°è¿”å›çš„ç‚¹
    :return: ç¬¬ä¸€ä¸ªè¿”å›æ˜¯flag = 0çš„ç‚¹ï¼Œç¬¬äºŒä¸ªæ˜¯flag = 1çš„ç‚¹
    '''
    flags = points[:,-1]
    flags = flags[:,np.newaxis]
    # åˆ›å»ºæ©ç  (axis=1è¡¨ç¤ºæ˜¯å¯¹è¡Œè¿›è¡Œåˆ¤æ–­ï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªå‘é‡)
    mask0 = np.any((flags == 0), axis=1)  # flag = 0
    mask1 = np.any((flags == 1), axis=1)  # flag = 1
    points = points[:,0:-1]

    return points[mask0], points[mask1]


def p42XYZ_ts(p4: torch.Tensor) -> torch.Tensor:
    '''
    åœ¨pytorchä¸­å®ç°çš„ä»äº”åŸºè‰²åæ ‡åˆ°XYZçš„è½¬æ¢

    :param p5: è¾“å…¥çš„äº”åŸºè‰²åæ ‡åæ ‡
    :return: XYZä¸‹çš„åæ ‡
    '''
    return torch.matmul(p4, XYZ4_ts)

def p52XYZ_ts(p5: torch.Tensor) -> torch.Tensor:
    '''
    åœ¨pytorchä¸­å®ç°çš„ä»äº”åŸºè‰²åæ ‡åˆ°XYZçš„è½¬æ¢

    :param p5: è¾“å…¥çš„äº”åŸºè‰²åæ ‡åæ ‡
    :return: XYZä¸‹çš„åæ ‡
    '''
    return torch.matmul(p5, XYZ5_ts)


def XYZ2LAB(xyz: torch.Tensor) -> torch.Tensor:
      '''
      è¾“å…¥XYZä¸‹åæ ‡å¾—åˆ°LABä¸‹åæ ‡

      :param xyz: è¾“å…¥XYZä¸‹åæ ‡
      :return: LABä¸‹çš„åæ ‡
      '''
      # æ±‚x_r, y_r, z_r
      xr = xyz[:,0] / ref_X
      yr = xyz[:,1] / ref_Y
      zr = xyz[:,2] / ref_Z

      epsilon = 216.0 / 24389.0
      kappa = 24389.0 / 27.0

      fx = torch.where(
            xr > epsilon, xr.pow(1.0 / 3.0),
            (kappa * xr + 16.0) / 116.0
      )

      fy = torch.where(
            yr > epsilon, yr.pow(1.0 / 3.0),
            (kappa * yr + 16.0) / 116.0
      )

      fz = torch.where(
            zr > epsilon, zr.pow(1.0 / 3.0),
            (kappa * zr + 16.0) / 116.0
      )

      # è®¡ç®—L*, a*, b*
      L = (116.0 * fy) - 16.0
      a = 500.0 * (fx - fy)
      b = 200.0 * (fy - fz)
      lab = torch.stack([L,a,b],dim=-1) # dim=-1è¡¨ç¤ºæ²¿ç€æœ€åä¸€ä¸ªç»´åº¦æ‹¼æ¥

      return lab

# æµ‹è¯•ä»£ç 
# xyz_white = torch.tensor([[0.95047, 1.00000, 1.08883]], dtype=torch.float32)
# lab_white = XYZ2LAB(xyz_white)
# print("D65 ç™½ç‚¹ â†’ Lab:", lab_white.cpu().numpy())

def CIE2000(Lab1: torch.Tensor, Lab2: torch.Tensor, kL: float = 1.0, kC: float = 1.0, kH: float = 1.0) -> torch.Tensor:
      # å–å‡ºæ•°æ®
      L1, a1, b1 = Lab1[:,0], Lab1[:,1], Lab1[:,2]
      L2, a2, b2 = Lab2[:,0], Lab2[:,1], Lab2[:,2]

      # è®¡ç®—C1,C2,Cå¹³å‡
      C1 = torch.sqrt(a1 * a1 + b1 * b1)  # (B,)
      C2 = torch.sqrt(a2 * a2 + b2 * b2)
      C_mean = (C1 + C2) / 2.0

      # è®¡ç®—G
      pow_Cmean_7 = torch.pow(C_mean, 7)
      pow_25_7 = torch.pow(torch.tensor(25.0, device=C_mean.device), 7)
      G = 0.5 * (1 - torch.sqrt(pow_Cmean_7 / (pow_Cmean_7 + pow_25_7)))

      # è®¡ç®—a1', a2'
      a1_pi = (1 + G) * a1
      a2_pi = (1 + G) * a2

      # è®¡ç®—C1', C2',Cå¹³å‡â€˜
      C1_pi = torch.sqrt(a1_pi * a1_pi + b1 * b1)
      C2_pi = torch.sqrt(a2_pi * a2_pi + b2 * b2)
      C_mean_pi = (C1_pi+C2_pi) /2.0

      # è®¡ç®—h1', h2'
      h1_pi = torch.where(
            torch.atan2(b1, a1_pi) >=0,torch.atan2(b1, a1_pi),
            torch.atan2(b1, a1_pi) + 2 * torch.pi
            )
      h2_pi = torch.where(
            torch.atan2(b2, a2_pi) >=0,torch.atan2(b2, a2_pi),
            torch.atan2(b2, a2_pi) + 2 * torch.pi
            )

      # è®¡ç®—hå¹³å‡â€™
      h1pi_h2pi = h1_pi - h2_pi
      h_mean_pi = torch.where(
            (C1_pi * C2_pi) == 0, h1_pi + h2_pi,
            torch.where(
                  h1pi_h2pi.abs() > torch.pi,(h1_pi + h2_pi + 2 * torch.pi) / 2,
                  (h1_pi + h2_pi) / 2)
            )

      # è®¡ç®—T
      T = (1
           - 0.17 * torch.cos(h_mean_pi - torch.pi / 6)
           + 0.24 * torch.cos(2 * h_mean_pi)
           + 0.32 * torch.cos(3 * h_mean_pi + torch.pi / 30)
           - 0.20 * torch.cos(4 * h_mean_pi - 63 * torch.pi / 180)
           )

      # è®¡ç®—delta h'
      h2pi_h1pi = h2_pi - h1_pi
      delta_h_pi = torch.where(
            (C1_pi * C2_pi) == 0,torch.zeros_like(h2pi_h1pi),
            torch.where(
                  h2pi_h1pi.abs() <= torch.pi, h2pi_h1pi,
                  h2pi_h1pi - 2 * torch.pi * torch.sign(h2pi_h1pi)
             )
      )

      dLp = L2 - L1
      dCp = C2_pi - C1_pi
      dHp = 2 * torch.sqrt(C1_pi * C2_pi) * torch.sin(delta_h_pi / 2.0)

      # è®¡ç®—åŠ æƒå‡½æ•° SL, SC, SH
      L_mean = (L1 + L2) / 2.0
      S_L = 1 + (0.015 * (L_mean - 50) ** 2) / torch.sqrt(20 + (L_mean - 50) ** 2)
      S_C = 1 + 0.045 * C_mean_pi
      S_H = 1 + 0.015 * C_mean_pi * T

      # è®¡ç®—delta theta
      delta_theta = (30 * torch.pi / 180) * torch.exp(-((h_mean_pi * 180 / torch.pi - 275) / 25) ** 2)

      # è®¡ç®—R_C, R_T
      R_C = 2 * torch.sqrt(pow_Cmean_7 / (pow_Cmean_7 + pow_25_7))
      R_T = -R_C * torch.sin(2 * delta_theta)

      # æœ€ç»ˆè®¡ç®— delta E
      delta_E = torch.sqrt(
            (dLp / (kL * S_L)) ** 2 +
            (dCp / (kC * S_C)) ** 2 +
            (dHp / (kH * S_H)) ** 2 +
            R_T * (dCp / (kC * S_C)) * (dHp / (kH * S_H))
      )

      return delta_E

def four2five_ts(cordinate: torch.Tensor, XYZ4_ts, XYZ5_ts):
    '''
    åœ¨pytorchä¸­ï¼Œåˆ©ç”¨ä¼ªé€†ï¼Œå°†å››åŸºè‰²ä¸‹çš„åæ ‡è½¬ä¸ºäº”åŸºè‰²ä¸‹çš„åæ ‡

    :param cordinate: (n,4) çš„çŸ©é˜µï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªå››åŸºè‰²ä¸‹çš„åæ ‡
    :param XYZ4: å››åŸºè‰²è½¬æ¢çŸ©é˜µ (3,4)
    :param XYZ5: äº”åŸºè‰²è½¬æ¢çŸ©é˜µ (3,5)
    :return: P_flags æ ‡è®°åçš„äº”åŸºè‰²åæ ‡
    '''
    # å…ˆå°†è¾“å…¥çš„å››åŸºè‰²é¢œè‰²åœ¨XYZä¸‹è¡¨ç¤º
    cordinate = cordinate.T
    mat4 = XYZ4_ts.T.detach().clone()
    xyz = torch.matmul(mat4,cordinate)  # cordiante æ˜¯(4,)çš„åˆ—å‘é‡

    # æ±‚äº”åŸºè‰²çŸ©é˜µçš„ä¼ªé€†
    mat5 = XYZ5_ts.T.detach().clone()
    mat5_inv = torch.linalg.pinv(mat5)
    P_5 = torch.matmul(mat5_inv, xyz)
    return P_5.T

# # ä½ çš„å‡½æ•°åä¸º CIE2000ï¼Œè¿™é‡Œä¸ºäº†ç»Ÿä¸€æµ‹è¯•å°è£…ä¸€ä¸‹
# def test_CIE2000():
#     # æµ‹è¯•ç”¨ä¾‹æ¥è‡ª Sharma et al. (2005)
#     test_cases = [
#         ([50.0000, 2.6772, -79.7751], [50.0000, 0.0000, -82.7485], 2.0425),
#         ([50.0000, 3.1571, -77.2803], [50.0000, 0.0000, -82.7485], 2.8615),
#         ([50.0000, 2.8361, -74.0200], [50.0000, 0.0000, -82.7485], 3.4412),
#     ]
#
#     Lab1 = torch.tensor([x[0] for x in test_cases], dtype=torch.float32)
#     Lab2 = torch.tensor([x[1] for x in test_cases], dtype=torch.float32)
#     expected = torch.tensor([x[2] for x in test_cases], dtype=torch.float32)
#
#     # è°ƒç”¨ä½ çš„å‡½æ•°
#     results = CIE2000(Lab1, Lab2)
#
#     # æ‰“å°å¯¹æ¯”ç»“æœ
#     for i, (res, exp) in enumerate(zip(results, expected)):
#         diff = abs(res.item() - exp.item())
#         print(f"Test {i+1}: Î”E00 = {res:.4f}, Expected = {exp:.4f}, Diff = {diff:.4f}")
#
#     # åˆ¤æ–­æ˜¯å¦å…¨éƒ¨é€šè¿‡
#     assert torch.allclose(results, expected, atol=1e-3), "æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥å®ç°"
#     print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
#
# # è¿è¡Œæµ‹è¯•
# test_CIE2000()

# å®šä¹‰æ•°æ®é›†
class GamutMp_Dataset(Dataset):
      def __init__(self, bt2020_pts: np.ndarray):
            """
            bt2020_pts: np.ndarray, shape=(N,3)ï¼Œçº¿æ€§åŒ– BT.2020 RGBï¼Œå‡åœ¨ [0,1] èŒƒå›´
            """
            super().__init__()
            self.x = torch.from_numpy(bt2020_pts).float()   # (N,3), dtype=torch.float32

      def __len__(self):
            return self.x.shape[0]

      def __getitem__(self, idx):
            # è¿”å› BT.2020 RGB
            return self.x[idx]

# å®šä¹‰ç¥ç»ç½‘ç»œ
class MLP_withflags(nn.Module):
      def __init__(self):
            super(MLP_withflags,self).__init__()
            self.proj = nn.Linear(5, 256)  # 1.æŠ•å½±å±‚
            self.net = nn.Sequential(
                  nn.Linear(256, 256),  # 2.éšè—å±‚1 256
                  nn.LayerNorm(256),
                  nn.GELU(),
                  # éšè—å±‚ 256 â†’ 256
                  nn.Linear(256, 256),  # 3.éšè—å±‚2 256
                  nn.LayerNorm(256),
                  nn.GELU(),
                  # éšè—å±‚ 256 â†’ 128
                  nn.Linear(256, 128),  # 4.éšè—å±‚3 128
                  nn.LayerNorm(128),
                  nn.GELU(),
                  # éšè—å±‚ 128 â†’ 64
                  nn.Linear(128, 64),   # 5.éšè—å±‚4 64
                  nn.LayerNorm(64),
                  nn.GELU(),
                  # éšè—å±‚ 64 â†’ 64
                  nn.Linear(64, 64),    # 6.éšè—å±‚5 64
                  nn.LayerNorm(64),
                  nn.GELU(),
                  # è¾“å‡ºå±‚ 64 â†’ 3ï¼Œå¹¶é™åˆ¶åˆ° [0,1]
                  nn.Linear(64, 5),
                  nn.Sigmoid()
            )
      def forward(self,x):
            out = self.proj(x)
            out = self.net(out)
            return out

class MLP_oss_only(nn.Module):
      def __init__(self):
            super(MLP_oss_only,self).__init__()
            self.proj = nn.Linear(4, 256)  # 1.æŠ•å½±å±‚
            self.net = nn.Sequential(
                  nn.Linear(256, 256),  # 2.éšè—å±‚1 256
                  nn.LayerNorm(256),
                  nn.GELU(),
                  # éšè—å±‚ 256 â†’ 256
                  nn.Linear(256, 256),  # 3.éšè—å±‚2 256
                  nn.LayerNorm(256),
                  nn.GELU(),
                  # éšè—å±‚ 256 â†’ 128
                  nn.Linear(256, 128),  # 4.éšè—å±‚3 128
                  nn.LayerNorm(128),
                  nn.GELU(),
                  # éšè—å±‚ 128 â†’ 64
                  nn.Linear(128, 64),   # 5.éšè—å±‚4 64
                  nn.LayerNorm(64),
                  nn.GELU(),
                  # éšè—å±‚ 64 â†’ 64
                  nn.Linear(64, 64),    # 6.éšè—å±‚5 64
                  nn.LayerNorm(64),
                  nn.GELU(),
                  # è¾“å‡ºå±‚ 64 â†’ 3ï¼Œå¹¶é™åˆ¶åˆ° [0,1]
                  nn.Linear(64, 5),
                  nn.Sigmoid()
            )
      def forward(self,x):
            out = self.proj(x)
            out = self.net(out)
            return out

# è®­ç»ƒä¸»æµç¨‹
huber = nn.HuberLoss(delta=1.0)
mse = torch.nn.MSELoss()
alpha = 0.8

def train_mlp_withflag(
    n_samples: int = 4096,
    batch_size: int = 1024,
    epochs: int = 20,
    lr: float = 1e-3,
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
):
      '''
      è¿™ä¸ªæ˜¯åŒæ—¶å°†è¶Šç•Œä¸éè¶Šç•Œç‚¹éƒ½æ‰“ä¸Šæ ‡è®°é€å…¥MLPçš„è®­ç»ƒè¿‡ç¨‹

      :param n_samples: å–æ ·ä¸ªæ•°ï¼Œé»˜è®¤4096
      :param batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤1024
      :param epochs: è®­ç»ƒè½®æ¬¡ï¼Œé»˜è®¤20
      :param lr: å­¦ä¹ ç‡ï¼Œé»˜è®¤1e-
      :param device: ä½¿ç”¨çš„è®¾å¤‡ï¼Œé»˜è®¤è‡ªåŠ¨åˆ¤åˆ«
      :return: model
      '''
      ## æ•°æ®é›†éƒ¨åˆ†
      # ç”Ÿæˆæ•°æ®é›†
      print(f'â–¶ æ­£åœ¨ä»å››åŸºè‰²è‰²å½©ç©ºé—´ä¸­é€‰å– {n_samples} ä¸ªç‚¹')

      # å–ç‚¹
      points4 = GetPoints4(n_samples=n_samples,seed=233)
      # ä½¿ç”¨æ±‚ä¼ªé€†çš„æ–¹æ³•
      points5 = four2five(points4, XYZ4, XYZ5)
      pt_flags4, pt_flags5 = flag(points4, points5)
      points_in5, points_oos5 = filter(pt_flags5) # å¯¹points5è¿›è¡Œåˆ†ç±»
      points_in4, points_oos4 = filter(pt_flags4) # å¯¹points4è¿›è¡Œåˆ†ç±»


      print(f'  å…±æœ‰{points_in5.shape[0]}ä¸ªç‚¹æ˜ å°„åä»åœ¨äº”åŸºè‰²ç©ºé—´å†…ï¼Œæœ‰{points_oos5.shape[0]}ä¸ªç‚¹æ˜ å°„åè¶…å‡ºç©ºé—´èŒƒå›´')
      print('â–¶ æ¥ä¸‹æ¥å°†å°è¯•å°†æ ‡è®°çš„å››åŸºè‰²åæ ‡é€å…¥MLP')



      # åŠ è½½è®­ç»ƒé›†
      perm = np.random.permutation(pt_flags4.shape[0])
      points_all = pt_flags4[perm]
      n_train = int(0.8 * points_all.shape[0]) #è®­ç»ƒé›†å¤§å°
      train_pts = points_all[:n_train]
      val_pts = points_all[n_train:]

      train_dts = GamutMp_Dataset(train_pts) # è®­ç»ƒé›†
      val_dts = GamutMp_Dataset(val_pts) #æµ‹è¯•é›†

      train_loader = DataLoader(train_dts, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
      val_loader = DataLoader(val_dts, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

      # -------------------------------

      ## å®šä¹‰æ¨¡å‹ã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨
      model = MLP_withflags().to(device)
      optimizer = optim.Adam(model.parameters(), lr=lr)
      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

      # -------------------------------

      # è®°å½•æŸå¤±ï¼Œç”¨äºç»˜å›¾
      train_losses = []
      val_losses = []
      fig, ax = plt.subplots()
      plt.ion()

      ## è®­ç»ƒéƒ¨åˆ†
      for e in range(1, epochs+1):
            model.train()
            running_loss = 0.0

            for p4 in train_loader:
                  p4 = p4.to(device) # ç§»åˆ°gpu
                  p4_noflag = p4[:, 0:-1]  # å…ˆæŠŠæ ‡è®°å»æ‰ï¼Œç”¨äºè¿›è¡Œåæ ‡å˜æ¢
                  p4_noflag = p4_noflag.to(device)

                  # å…ˆæ±‚å‡ºp4ï¼ˆåŸå§‹çš„å››åŸºè‰²ä¸‹çš„åæ ‡ï¼‰çš„LABåæ ‡
                  XYZ_org = p42XYZ_ts(p4_noflag)
                  LAB_org = XYZ2LAB(XYZ_org)
                  # æ±‚å‡ºè‹¥ç›´æ¥æŠ•å½±åˆ°è¾¹ç•Œçš„åæ ‡
                  direct_pro_t = torch.clamp(four2five_ts(p4_noflag,XYZ4_ts, XYZ5_ts),0,1)
                  direct_pro_t_xyz = p52XYZ_ts(direct_pro_t)
                  direct_pro_t_lab = XYZ2LAB(direct_pro_t_xyz)

                  # æ­£å‘ä¼ æ’­,å¾—åˆ°(batch_size,5)çš„åŒ…å«æ˜ å°„åçš„äº”åŸºè‰²ä¸‹åæ ‡çš„çŸ©é˜µ
                  p5_lin = model(p4)

                  # è½¬æ¢p5åæ ‡ä¸ºLAB
                  XYZ_p5 = p52XYZ_ts(p5_lin)
                  LAB_p5 = XYZ2LAB(XYZ_p5)


                  # è®¡ç®— CIEDE2000 delta E
                  delta_E = CIE2000(LAB_p5,LAB_org)
                  loss_de = huber(delta_E,torch.zeros_like(delta_E))
                  loss_m =mse(LAB_p5,direct_pro_t_lab)
                  loss = torch.mean(alpha * loss_de + (1 - alpha) * loss_m)

                  # åå‘ä¼ æ’­
                  optimizer.zero_grad() # æ¸…é™¤å·²æœ‰æ¢¯åº¦
                  loss.backward()
                  optimizer.step() # æ›´æ–°å‚æ•°

                  running_loss += loss.item() * p4.size(0) # å•ä¸ªbatchçš„lossæ€»å’Œ


            train_e_loss = running_loss / len(train_dts)
            train_losses.append(train_e_loss)

            ## éªŒè¯éƒ¨åˆ†
            model.eval()
            val_running_loss = 0.0
            with torch.no_grad():
                  for p4 in val_loader:
                        p4 = p4.to(device)
                        p4_noflag = p4[:, 0:-1]  # å…ˆæŠŠæ ‡è®°å»æ‰ï¼Œç”¨äºè¿›è¡Œåæ ‡å˜æ¢
                        p4_noflag = p4_noflag.to(device)

                        # å…ˆæ±‚å‡ºp4ï¼ˆåŸå§‹çš„å››åŸºè‰²ä¸‹çš„åæ ‡ï¼‰çš„LABåæ ‡
                        XYZ_org = p42XYZ_ts(p4_noflag)
                        LAB_org = XYZ2LAB(XYZ_org)
                        # æ±‚å‡ºè‹¥ç›´æ¥æŠ•å½±åˆ°è¾¹ç•Œçš„åæ ‡
                        direct_pro_v = torch.clamp(four2five_ts(p4_noflag, XYZ4_ts, XYZ5_ts), 0, 1)
                        direct_pro_v_xyz = p52XYZ_ts(direct_pro_v)
                        direct_pro_v_lab = XYZ2LAB(direct_pro_v_xyz)

                        # æ­£å‘ä¼ æ’­,å¾—åˆ°(batch_size,5)çš„åŒ…å«æ˜ å°„åçš„äº”åŸºè‰²ä¸‹åæ ‡çš„çŸ©é˜µ
                        p5_lin = model(p4)

                        # è½¬æ¢p5åæ ‡ä¸ºLAB
                        XYZ_p5 = p52XYZ_ts(p5_lin)
                        LAB_p5 = XYZ2LAB(XYZ_p5)


                        # è®¡ç®— CIEDE2000 delta E
                        delta_E = CIE2000(LAB_p5, LAB_org)
                        loss_de_val = huber(delta_E, torch.zeros_like(delta_E))
                        loss_m_val = mse(LAB_p5, direct_pro_v_lab)
                        loss_val = torch.mean(alpha * loss_de_val + (1 - alpha) * loss_m_val)
                        val_running_loss += loss_val.item() * p4.size(0)

            val_e_loss = val_running_loss / len(val_dts)
            val_losses.append(val_e_loss)
            scheduler.step(val_e_loss)

            print(f'â–¶ Epoch [{e:02d}/{epochs:02d}]  Train Î”Eâ‚€â‚€: {train_e_loss:.4f}   Val Î”Eâ‚€â‚€: {val_e_loss:.4f}')
            # ç»˜å›¾
            x_vals = list(range(1, len(train_losses) + 1))
            ax.clear()
            ax.plot(x_vals,train_losses, label='Train Loss', color='blue')
            ax.plot(x_vals,val_losses, label='Validation Loss', color='orange')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.set_title('Training Progress')
            ax.legend()
            ax.grid(True)
            plt.pause(0.3)  # æš‚åœ0.3ç§’æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´

      plt.ioff()
      plt.show()

      return model

def train_mlp_oos_only(
    n_samples: int = 4096,
    batch_size: int = 1024,
    epochs: int = 20,
    lr: float = 1e-3,
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
):
      '''
      è¿™ä¸ªæ˜¯ä»…å°†è¶Šç•Œç‚¹ï¼ˆflag = 1ï¼‰é€å…¥MLPçš„è®­ç»ƒè¿‡ç¨‹

      :param n_samples: å–æ ·ä¸ªæ•°ï¼Œé»˜è®¤4096
      :param batch_size: æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤1024
      :param epochs: è®­ç»ƒè½®æ¬¡ï¼Œé»˜è®¤20
      :param lr: å­¦ä¹ ç‡ï¼Œé»˜è®¤1e-
      :param device: ä½¿ç”¨çš„è®¾å¤‡ï¼Œé»˜è®¤è‡ªåŠ¨åˆ¤åˆ«
      :return: model
      '''
      ## æ•°æ®é›†éƒ¨åˆ†
      # ç”Ÿæˆæ•°æ®é›†
      print(f'â–¶ æ­£åœ¨ä»å››åŸºè‰²è‰²å½©ç©ºé—´ä¸­é€‰å– {n_samples} ä¸ªç‚¹')

      # å–ç‚¹
      points4 = GetPoints4(n_samples=n_samples,seed=233)
      # ä½¿ç”¨æ±‚ä¼ªé€†çš„æ–¹æ³•
      points5 = four2five(points4, XYZ4, XYZ5)
      pt_flags4, pt_flags5 = flag(points4, points5)
      points_in5, points_oos5 = filter(pt_flags5) # å¯¹points5è¿›è¡Œåˆ†ç±»
      points_in4, points_oos4 = filter(pt_flags4) # å¯¹points4è¿›è¡Œåˆ†ç±»


      print(f'  å…±æœ‰{points_in5.shape[0]}ä¸ªç‚¹æ˜ å°„åä»åœ¨äº”åŸºè‰²ç©ºé—´å†…ï¼Œæœ‰{points_oos5.shape[0]}ä¸ªç‚¹æ˜ å°„åè¶…å‡ºç©ºé—´èŒƒå›´')
      print('â–¶ æ¥ä¸‹æ¥å°†åªå°†æ˜ å°„åè¶Šç•Œçš„å››åŸºè‰²åæ ‡é€å…¥MLP')



      # åŠ è½½è®­ç»ƒé›†
      perm = np.random.permutation(points_oos4.shape[0])
      points_oos = points_oos4[perm]
      n_train = int(0.8 * points_oos.shape[0]) #è®­ç»ƒé›†å¤§å°
      train_pts = points_oos[:n_train]
      val_pts = points_oos[n_train:]

      train_dts = GamutMp_Dataset(train_pts) # è®­ç»ƒé›†
      val_dts = GamutMp_Dataset(val_pts) #æµ‹è¯•é›†

      train_loader = DataLoader(train_dts, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
      val_loader = DataLoader(val_dts, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

      # -------------------------------

      ## å®šä¹‰æ¨¡å‹ã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨
      model = MLP_oss_only().to(device)
      optimizer = optim.Adam(model.parameters(), lr=lr)
      scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

      # -------------------------------

      # è®°å½•æŸå¤±ï¼Œç”¨äºç»˜å›¾
      train_losses = []
      val_losses = []
      fig, ax = plt.subplots()
      plt.ion()

      ## è®­ç»ƒéƒ¨åˆ†
      for e in range(1, epochs+1):
            model.train()
            running_loss = 0.0

            for p4 in train_loader:
                  p4 = p4.to(device) # ç§»åˆ°gpu

                  # å…ˆæ±‚å‡ºp4ï¼ˆåŸå§‹çš„å››åŸºè‰²ä¸‹çš„åæ ‡ï¼‰çš„LABåæ ‡
                  XYZ_org = p42XYZ_ts(p4)
                  LAB_org = XYZ2LAB(XYZ_org)
                  # æ±‚å‡ºè‹¥ç›´æ¥æŠ•å½±åˆ°è¾¹ç•Œçš„åæ ‡
                  direct_pro_t = torch.clamp(four2five_ts(p4,XYZ4_ts, XYZ5_ts),0,1)
                  direct_pro_t_xyz = p52XYZ_ts(direct_pro_t)
                  direct_pro_t_lab = XYZ2LAB(direct_pro_t_xyz)

                  # æ­£å‘ä¼ æ’­,å¾—åˆ°(batch_size,5)çš„åŒ…å«æ˜ å°„åçš„äº”åŸºè‰²ä¸‹åæ ‡çš„çŸ©é˜µ
                  p5_lin = model(p4)

                  # è½¬æ¢p5åæ ‡ä¸ºLAB
                  XYZ_p5 = p52XYZ_ts(p5_lin)
                  LAB_p5 = XYZ2LAB(XYZ_p5)


                  # è®¡ç®— CIEDE2000 delta E
                  delta_E = CIE2000(LAB_p5,LAB_org)
                  loss_de = huber(delta_E,torch.zeros_like(delta_E))
                  loss_m =mse(LAB_p5,direct_pro_t_lab)
                  loss = torch.mean(alpha * loss_de + (1 - alpha) * loss_m)

                  # åå‘ä¼ æ’­
                  optimizer.zero_grad() # æ¸…é™¤å·²æœ‰æ¢¯åº¦
                  loss.backward()
                  optimizer.step() # æ›´æ–°å‚æ•°

                  running_loss += loss.item() * p4.size(0) # å•ä¸ªbatchçš„lossæ€»å’Œ

            train_e_loss = running_loss / len(train_dts)
            train_losses.append(train_e_loss)

            ## éªŒè¯éƒ¨åˆ†
            model.eval()
            val_running_loss = 0.0
            with torch.no_grad():
                  for p4 in val_loader:
                        p4 = p4.to(device)

                        # å…ˆæ±‚å‡ºp4ï¼ˆåŸå§‹çš„å››åŸºè‰²ä¸‹çš„åæ ‡ï¼‰çš„LABåæ ‡
                        XYZ_org = p42XYZ_ts(p4)
                        LAB_org = XYZ2LAB(XYZ_org)
                        # æ±‚å‡ºè‹¥ç›´æ¥æŠ•å½±åˆ°è¾¹ç•Œçš„åæ ‡
                        direct_pro_v = torch.clamp(four2five_ts(p4, XYZ4_ts, XYZ5_ts), 0, 1)
                        direct_pro_v_xyz = p52XYZ_ts(direct_pro_v)
                        direct_pro_v_lab = XYZ2LAB(direct_pro_v_xyz)

                        # æ­£å‘ä¼ æ’­,å¾—åˆ°(batch_size,5)çš„åŒ…å«æ˜ å°„åçš„äº”åŸºè‰²ä¸‹åæ ‡çš„çŸ©é˜µ
                        p5_lin = model(p4)

                        # è½¬æ¢p5åæ ‡ä¸ºLAB
                        XYZ_p5 = p52XYZ_ts(p5_lin)
                        LAB_p5 = XYZ2LAB(XYZ_p5)


                        # è®¡ç®— CIEDE2000 delta E
                        delta_E = CIE2000(LAB_p5, LAB_org)
                        loss_de_val = huber(delta_E, torch.zeros_like(delta_E))
                        loss_m_val = mse(LAB_p5, direct_pro_v_lab)
                        loss_val = torch.mean(alpha * loss_de_val + (1 - alpha) * loss_m_val)
                        val_running_loss += loss_val.item() * p4.size(0)

            val_e_loss = val_running_loss / len(val_dts)
            val_losses.append(val_e_loss)
            scheduler.step(val_e_loss)

            print(f'â–¶ Epoch [{e:02d}/{epochs:02d}]  Train Î”Eâ‚€â‚€: {train_e_loss:.4f}   Val Î”Eâ‚€â‚€: {val_e_loss:.4f}')
            # ç»˜å›¾
            x_vals = list(range(1, len(train_losses) + 1))
            ax.clear()
            ax.plot(x_vals,train_losses, label='Train Loss', color='blue')
            ax.plot(x_vals,val_losses, label='Validation Loss', color='orange')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.set_title('Training Progress')
            ax.legend()
            ax.grid(True)
            plt.pause(0.3)  # æš‚åœ0.3ç§’æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´

      plt.ioff()
      plt.show()

      return model


def train():
      print(f'âœ¨ä½¿ç”¨è®¾å¤‡: {device}')
      print(f'â–¶ è®­ç»ƒå¼€å§‹')

      n_samples = 4096000
      batch_size = 10240
      epochs = 20
      lr = 0.001

      print('1.æ‰€æœ‰ç‚¹æ ‡è®°åé€å…¥MLP   2.ä»…é€å…¥è¶Šç•Œç‚¹')
      choose = int(input('è¯·é€‰æ‹©ä¸€ç§è®­ç»ƒæ–¹å¼: [1/2]'))
      if choose == 1:
            model = train_mlp_withflag(
                  n_samples=n_samples,
                  batch_size=batch_size,
                  epochs=epochs,
                  lr=lr,
                  device=device
            )

            # ä¿å­˜æ¨¡å‹
            file_name = f'models/Q2/all/{datetime.strftime(datetime.now(), "%Y%m%d_%H%M%S")}.pth'
            torch.save(model.state_dict(), file_name)

      elif choose == 2:
            model = train_mlp_oos_only(
                  n_samples=n_samples,
                  batch_size=batch_size,
                  epochs=epochs,
                  lr=lr,
                  device=device
            )

            # ä¿å­˜æ¨¡å‹
            file_name = f'models/Q2/oss/{datetime.strftime(datetime.now(), "%Y%m%d_%H%M%S")}.pth'
            torch.save(model.state_dict(), file_name)


def project_mlp(ckpt_path, p4, network):
      '''
      æ­¤å‡½æ•°ç”¨äºåŠ è½½æ¨¡å‹å¹¶æ¨ç†

      :param ckpt_path: æ¨¡å‹çš„è·¯å¾„
      :param p4: è¾“å…¥å››åŸºè‰²ä¸‹çš„åæ ‡
      :param nn: ä¼ å…¥ç¥ç»ç½‘ç»œçš„ç±»å
      :return: äº”åŸºè‰²ä¸‹çš„åæ ‡
      '''
      device = 'cuda' if torch.cuda.is_available() else 'cpu'
      model = network().to(device)
      # åˆ›å»º Huber ä¸mse æŸå¤±å‡½æ•°ï¼Œä¸è¿›è¡Œèšåˆ
      huber = nn.SmoothL1Loss(reduction='none')
      mse = nn.MSELoss(reduction='none')

      checkpoint_pth = ckpt_path
      state_dict = torch.load(checkpoint_pth, map_location=device)
      model.load_state_dict(state_dict)
      p4 = torch.from_numpy(p4).float()
      p4 = p4.to(device)


      model.eval()
      with torch.no_grad():
            project_p5_t = model(p4)
            project_p5 = project_p5_t.cpu().numpy()  # è½¬ä¸ºnumpyæ•°ç»„

            # è®¡ç®—loss
            if p4.shape[1] == 5:
                  p4 = p4[:,0:-1]
            XYZ_org = p42XYZ_ts(p4)  # å°†è¦æ˜ å°„çš„å››åŸºè‰²ä¸‹çš„åæ ‡è½¬ä¸ºXYZåæ ‡
            LAB_org = XYZ2LAB(XYZ_org)  # å°†è¦æ˜ å°„çš„XYZåæ ‡è½¬ä¸ºLABåæ ‡
            direct_pro_v = torch.clamp(four2five_ts(p4, XYZ4_ts, XYZ5_ts), 0, 1)  # æ±‚ç›´æ¥æ˜ å°„çš„åæ ‡
            direct_pro_v_xyz = p52XYZ_ts(direct_pro_v)
            direct_pro_v_lab = XYZ2LAB(direct_pro_v_xyz)
            XYZ_p5 = p52XYZ_ts(project_p5_t)  # æ˜ å°„åäº”åŸºè‰²ä¸‹çš„åæ ‡è½¬ä¸ºXYZåæ ‡
            LAB_p5 = XYZ2LAB(XYZ_p5)  # æ˜ å°„åXYZåæ ‡è½¬ä¸ºLABåæ ‡
            delta_E = CIE2000(LAB_p5, LAB_org)  # æ±‚CIEDE2000
            loss_de = huber(delta_E, torch.zeros_like(delta_E))
            loss_m = mse(LAB_p5, direct_pro_v_lab).mean(dim=1)
            loss = alpha * loss_de + (1 - alpha) * loss_m  # lossç”±CIEDE2000ä¸MSEåŠ æƒæ±‚å¾—
            loss = loss.cpu().numpy()

      return project_p5, loss

def project(p_num=1000):

      proj_pts = GetPoints4(p_num)  # å…ˆå–ç‚¹
      ## æœ‰æ ‡è®° -----------<begin>-----------
      print("ğŸš© æ–¹æ³•1ï¼šå¸¦æ ‡è®°çš„çº¯MLPæ˜ å°„æ–¹æ³•")
      ### æ ‡è®°ï¼Œåˆ’åˆ†è¶Šç•Œä¸éè¶Šç•Œç‚¹
      proj_pts_pinv = four2five(proj_pts, XYZ4, XYZ5)  # æœ€å°äºŒä¹˜æ³•
      proj_pts_flags4, proj_pts_flags5 = flag(proj_pts, proj_pts_pinv)  # æ ‡è®°

      flag0_pts, flag1_pts = filter(proj_pts_flags4)  # éœ€è¦ä¼ å…¥mlpçš„å››åŸºè‰²ç‚¹
      print(f'å…±æœ‰{flag1_pts.shape[0]}ä¸ªç‚¹è¶Šç•Œï¼Œ')
      ckpt_path = "models/Q2/all/20250524_160416.pth"  # æ¨¡å‹è·¯å¾„
      ### æ˜ å°„
      pjt_mlp_flags, loss_mlp = project_mlp(ckpt_path, proj_pts_flags4, MLP_withflags)  # è¶Šç•Œç‚¹çš„äº”åŸºè‰²åæ ‡
      ### æ±‚moss
      #### åªæœ‰MLPçš„loss
      print('ğŸ°ä»¥ä¸‹æ˜¯MLPçš„lossï¼Œä¹Ÿæ˜¯æ•´ä½“çš„loss')
      loss_mlp_95 = np.percentile(loss_mlp, 95, 0)
      loss_mlp_99 = np.percentile(loss_mlp, 99, 0)
      loss_mlp_mean = np.mean(loss_mlp, axis=0)
      # print("â¤ï¸ æ˜ å°„ç»“æœ:\n", pjt_mlp)
      print(f'æ•´ä½“çš„losså€¼çš„95åˆ†ä½æ•°ä¸º: {loss_mlp_95}')
      print(f'æ•´ä½“çš„losså€¼çš„99åˆ†ä½æ•°ä¸º: {loss_mlp_99}')
      print(f'æ•´ä½“çš„å¹³å‡lossä¸º: {loss_mlp_mean}')
      ## æœ‰æ ‡è®° -----------<end>-----------

      ## æ— æ ‡è®° -----------<begin>-----------
      print("ğŸš© æ–¹æ³•2ï¼šæœ€å°äºŒä¹˜æ³•+MLPæ˜ å°„æ–¹æ³•")
      ### æ ‡è®°ï¼Œåˆ’åˆ†è¶Šç•Œä¸éè¶Šç•Œç‚¹
      proj_pts_pinv = four2five(proj_pts, XYZ4, XYZ5)  # æœ€å°äºŒä¹˜æ³•
      proj_pts_flags4, proj_pts_flags5 = flag(proj_pts, proj_pts_pinv)  # æ ‡è®°

      _, mlp_pts = filter(proj_pts_flags4)  # éœ€è¦ä¼ å…¥mlpçš„å››åŸºè‰²ç‚¹
      print(f'å…±æœ‰{mlp_pts.shape[0]}ä¸ªç‚¹è¶Šç•Œ')
      ckpt_path = "models/Q2/oss/20250524_162828.pth"  # æ¨¡å‹è·¯å¾„
      ### æ˜ å°„
      direct_pts, _ = filter(proj_pts_flags5)  # æ²¡æœ‰è¶Šç•Œçš„ç‚¹çš„äº”åŸºè‰²ä¸‹çš„åæ ‡
      pjt_mlp, loss_mlp = project_mlp(ckpt_path, proj_pts, MLP_oss_only)  # è¶Šç•Œç‚¹çš„äº”åŸºè‰²åæ ‡
      ### æ±‚moss
      #### æ±‚æœ€å°äºŒä¹˜æ³•çš„loss
      xyz_direct_pts = p52XYZ(direct_pts)  # è½¬ä¸ºXYZ
      lab_direct_pts = colour.XYZ_to_Lab(xyz_direct_pts)  # è½¬ä¸ºLAB
      direct_origin, _ = filter(proj_pts_flags4)  # æ˜ å°„å‰çš„åæ ‡
      xyz_direct_origin = p42XYZ(direct_origin)  # æ˜ å°„å‰çš„åæ ‡
      lab_direct_origin = colour.XYZ_to_Lab(xyz_direct_origin)  # æ˜ å°„å‰çš„åæ ‡
      deltaE_direct = colour.difference.delta_E_CIE2000(lab_direct_pts, lab_direct_origin)
      loss_direct = alpha * deltaE_direct  # ç›´æ¥æ˜ å°„è¿‡ç¨‹ä¸­MSE = 0ï¼Œå› ä¸ºæ­¤å¤„çš„MSEæ˜¯ç›´æ¥æ˜ å°„ä¸ä½¿ç”¨MLPæ˜ å°„ç›´æ¥çš„MSE

      #### ä»…å…³æ³¨MLPçš„loss
      print('ğŸ°1.ä»¥ä¸‹æ˜¯MLPçš„loss')
      loss_mlp_95 = np.percentile(loss_mlp, 95, 0)
      loss_mlp_mean = np.mean(loss_mlp, axis=0)
      # print("â¤ï¸ æ˜ å°„ç»“æœ:\n", pjt_mlp)
      print(f'MLPçš„losså€¼çš„95åˆ†ä½æ•°ä¸º: {loss_mlp_95}')
      print(f'MLPçš„å¹³å‡lossä¸º: {loss_mlp_mean}{',å¯è®¤ä¸ºæ˜¯0' if loss_mlp_mean < 1e-6 else ''}')

      print('ğŸ°2.ä»¥ä¸‹æ˜¯æ˜ å°„åæ²¡æœ‰è¶Šç•Œçš„ç‚¹çš„lossï¼ˆè¿™äº›ç‚¹ç›´æ¥ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ï¼‰')
      print(f'æœªè¶Šç•Œç‚¹çš„å¹³å‡lossä¸º: {np.mean(loss_direct, axis=0)}')

      print('ğŸ°3.ä»¥ä¸‹æ˜¯åŠ ä¸Šæ²¡æœ‰è¶Šç•Œçš„ç‚¹çš„æ•´ä½“loss')
      loss_all = np.concatenate([loss_direct, loss_mlp])
      loss_all_mean = np.mean(loss_all, axis=0)
      loss_all_95 = np.percentile(loss_all, 95, 0)
      print(f'æ€»ä½“çš„losså€¼çš„95åˆ†ä½æ•°ä¸º: {loss_all_95}')
      print(f'æ€»ä½“çš„å¹³å‡lossä¸º: {loss_all_mean}')

      ## æ— æ ‡è®° -----------<end>-----------


# main
if __name__ == "__main__":
      device = 'cuda' if torch.cuda.is_available() else 'cpu'

      four_bases = np.array([430, 480, 550, 625])
      five_bases = np.array([440, 490, 530, 580, 610])
      XYZ4 = wavelength_2_xyz(four_bases)
      XYZ5 = wavelength_2_xyz(five_bases)
      # plot_xyz_color_vectors(XYZ5, five_bases)
      # plot_gamut_on_chromaticity_diagram(XYZ5,five_bases)

      # D65 ç™½ç‚¹çš„ XYZ å€¼ï¼ˆå½’ä¸€åŒ–å Yn=1.0ï¼‰
      ref_X = 0.95047
      ref_Y = 1.00000
      ref_Z = 1.08883

      XYZ4_ts = torch.tensor(XYZ4,dtype=torch.float32).to(device)
      XYZ5_ts = torch.tensor(XYZ5,dtype=torch.float32).to(device)

      # è®­ç»ƒ
      # train()

      # æ¨ç†
      project(p_num=1000)



